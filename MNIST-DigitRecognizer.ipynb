{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing useful libraries\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPool2D,Dropout,Dense,Flatten\n",
    "from keras.preprocessing import image\n",
    "import cv2\n",
    "from keras.utils import np_utils\n",
    "from sklearn.utils import shuffle\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dataset and dividing them in training and testing data\n",
    "train_set=pd.read_csv('.\\\\digit-recognizer\\\\train.csv')\n",
    "test_set=pd.read_csv('.\\\\digit-recognizer\\\\test.csv')\n",
    "Y=train_set['label']\n",
    "X=train_set.drop(columns='label')\n",
    "X=np.array(X).reshape(-1,28,28)\n",
    "Y=np.array(Y).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d02aec9508>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "image_index = 0 # You may select anything up to 42,000\n",
    "print(Y[image_index]) # The label is 1\n",
    "plt.imshow(X[image_index], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_cv,y_train,y_cv=train_test_split(X,Y,test_size=0.3,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12600, 28, 28)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=np_utils.to_categorical(y_train,num_classes=10)\n",
    "y_cv=np_utils.to_categorical(y_cv,num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (29400, 28, 28, 1)\n",
      "Number of images in x_train 29400\n",
      "Number of images in x_test 12600\n"
     ]
    }
   ],
   "source": [
    "# Reshaping the array to 4-dims so that it can work with the Keras API\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_cv = X_cv.reshape(X_cv.shape[0], 28, 28, 1)\n",
    "input_shape = (28, 28, 1)\n",
    "# Making sure that the values are float so that we can get decimal points after division\n",
    "X_train = X_train.astype('float32')\n",
    "X_cv = X_cv.astype('float32')\n",
    "# Normalizing the RGB codes by dividing it to the max RGB value.\n",
    "X_train /= 255\n",
    "X_cv /= 255\n",
    "print('x_train shape:', X_train.shape)\n",
    "print('Number of images in x_train', X_train.shape[0])\n",
    "print('Number of images in x_test', X_cv.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 10, 10, 16)        4624      \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 8, 8, 16)          2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 2, 2, 64)          9280      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               33280     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 64,202\n",
      "Trainable params: 64,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# creating the model\n",
    "model=Sequential()\n",
    "model.add(Conv2D(filters=32,kernel_size=(3,3),activation=\"relu\",input_shape=(28,28,1)))\n",
    "model.add(Conv2D(filters=32,kernel_size=(3,3),activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(filters=16,kernel_size=(3,3),activation=\"relu\"))\n",
    "model.add(Conv2D(filters=16,kernel_size=(3,3),activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(filters=64,kernel_size=(3,3),activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512,activation=\"relu\"))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(10,activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"RMSprop\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nigam\\Anaconda3\\envs\\gputest\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 29400 samples, validate on 12600 samples\n",
      "Epoch 1/20\n",
      "29400/29400 [==============================] - 18s 629us/step - loss: 0.7498 - accuracy: 0.7493 - val_loss: 0.2206 - val_accuracy: 0.9319\n",
      "Epoch 2/20\n",
      "29400/29400 [==============================] - 6s 209us/step - loss: 0.2416 - accuracy: 0.9250 - val_loss: 0.1214 - val_accuracy: 0.9623\n",
      "Epoch 3/20\n",
      "29400/29400 [==============================] - 6s 212us/step - loss: 0.1617 - accuracy: 0.9498 - val_loss: 0.0733 - val_accuracy: 0.9771\n",
      "Epoch 4/20\n",
      "29400/29400 [==============================] - 7s 223us/step - loss: 0.1236 - accuracy: 0.9620 - val_loss: 0.0701 - val_accuracy: 0.9784\n",
      "Epoch 5/20\n",
      "29400/29400 [==============================] - 7s 224us/step - loss: 0.1065 - accuracy: 0.9674 - val_loss: 0.0565 - val_accuracy: 0.9818\n",
      "Epoch 6/20\n",
      "29400/29400 [==============================] - 7s 227us/step - loss: 0.0902 - accuracy: 0.9731 - val_loss: 0.0761 - val_accuracy: 0.9753\n",
      "Epoch 7/20\n",
      "29400/29400 [==============================] - 7s 224us/step - loss: 0.0822 - accuracy: 0.9749 - val_loss: 0.0474 - val_accuracy: 0.9856\n",
      "Epoch 8/20\n",
      "29400/29400 [==============================] - 7s 227us/step - loss: 0.0748 - accuracy: 0.9776 - val_loss: 0.0593 - val_accuracy: 0.9821\n",
      "Epoch 9/20\n",
      "29400/29400 [==============================] - 7s 227us/step - loss: 0.0681 - accuracy: 0.9793 - val_loss: 0.0481 - val_accuracy: 0.9853\n",
      "Epoch 10/20\n",
      "29400/29400 [==============================] - 7s 226us/step - loss: 0.0655 - accuracy: 0.9805 - val_loss: 0.0471 - val_accuracy: 0.9854\n",
      "Epoch 11/20\n",
      "29400/29400 [==============================] - 7s 226us/step - loss: 0.0632 - accuracy: 0.9820 - val_loss: 0.0426 - val_accuracy: 0.9859\n",
      "Epoch 12/20\n",
      "29400/29400 [==============================] - 7s 227us/step - loss: 0.0561 - accuracy: 0.9824 - val_loss: 0.0376 - val_accuracy: 0.9883\n",
      "Epoch 13/20\n",
      "29400/29400 [==============================] - 7s 226us/step - loss: 0.0556 - accuracy: 0.9835 - val_loss: 0.0440 - val_accuracy: 0.9883\n",
      "Epoch 14/20\n",
      "29400/29400 [==============================] - 7s 227us/step - loss: 0.0508 - accuracy: 0.9838 - val_loss: 0.0392 - val_accuracy: 0.9885\n",
      "Epoch 15/20\n",
      "29400/29400 [==============================] - 7s 227us/step - loss: 0.0522 - accuracy: 0.9844 - val_loss: 0.0373 - val_accuracy: 0.9889\n",
      "Epoch 16/20\n",
      "29400/29400 [==============================] - 7s 226us/step - loss: 0.0493 - accuracy: 0.9845 - val_loss: 0.0342 - val_accuracy: 0.9900\n",
      "Epoch 17/20\n",
      "29400/29400 [==============================] - 7s 226us/step - loss: 0.0436 - accuracy: 0.9870 - val_loss: 0.0379 - val_accuracy: 0.9892\n",
      "Epoch 18/20\n",
      "29400/29400 [==============================] - 7s 226us/step - loss: 0.0456 - accuracy: 0.9858 - val_loss: 0.0360 - val_accuracy: 0.9887\n",
      "Epoch 19/20\n",
      "29400/29400 [==============================] - 7s 227us/step - loss: 0.0422 - accuracy: 0.9872 - val_loss: 0.0331 - val_accuracy: 0.9898\n",
      "Epoch 20/20\n",
      "29400/29400 [==============================] - 7s 226us/step - loss: 0.0412 - accuracy: 0.9878 - val_loss: 0.0444 - val_accuracy: 0.9883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1d02b60fb08>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train,y=y_train,batch_size=125,epochs=20,validation_data=(X_cv,y_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "model_json = model.to_json()\n",
    "with open(\"mnist_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"mnist_weights.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
